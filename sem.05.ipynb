{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Скрытые марковские модели\n",
    "Курс NLP, семинар 5.  \n",
    "Осень 2015.\n",
    "\n",
    "В данном задании вы реализуете:\n",
    "\n",
    "- Obvious POS-tagging\n",
    "- Contextless POS-tagging\n",
    "- Viterbi POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подпись**: *(введите сюда свои ФИО + отделение + курс)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import conll2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном семинаре мы будем решать задачу [POS (Part Of Speech)-tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging)'а, используя несколько подходов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очевидный POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала напишем простейший POS-теггер. У каждого слова определим какими тегами оно бывает отмечено в обучающем корпусе, а для предсказания выбираем наиболее частотный тег. Формула может быть записана следующим образом:\n",
    "$$\n",
    "    tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(tag_i \\mid w).\n",
    "$$\n",
    "Достаточно крупный минус данного метода заключается в том, что если какое-то слово в обучающем корпусе мы не встретили, то тег на нем никаким образом уже поставить не сможем. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуйте:\n",
    "1. Класс **SimpleNormalizer**, осуществляющий обычную нормализацию частот Counter'ов, превращающих их в вероятностное распределение.\n",
    "2. Класс **WordTagModel**, хранящий для каждого слова вероятности быть отмеченным тем или иным тегом.\n",
    "3. Класс **ObviousPOSTagger**, сопоставляющий последовательности слов последовательность тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseNormalizer:\n",
    "    def normalize(self, counter):\n",
    "        ### YOUR CODE HERE\n",
    "        # Normalize counter to get a probability distribution.\n",
    "        freqs = [counter[x] for x in counter]\n",
    "        S = sum(freqs)\n",
    "        probs = {}\n",
    "        for x in counter:\n",
    "            probs[x] = counter[x]/S\n",
    "        return probs\n",
    "        ### END YOUR CODE\n",
    "\n",
    "class WordTagModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        ### YOUR CODE HERE\n",
    "        # For each word add Counter with tags corresponding to this word.\n",
    "        # Don't forget to normalize your counters!\n",
    "        tuples = []\n",
    "        for x in tagged_sents:\n",
    "            tuples+= x\n",
    "        counted_wt_pairs = Counter(tuples)\n",
    "        model = Counter()\n",
    "        for pair in counted_wt_pairs:\n",
    "            word = pair[0]\n",
    "            model[word] = {} \n",
    "        \n",
    "        for pair in counted_wt_pairs:\n",
    "            word = pair[0]\n",
    "            model[word][pair[1]] = counted_wt_pairs[pair]\n",
    "        \n",
    "        for x in model:\n",
    "            model[x] = self.normalizer.normalize(model[x])\n",
    "            \n",
    "        self.model = model\n",
    "            \n",
    "        ### END YOUR CODE\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.model[word]\n",
    "\n",
    "    \n",
    "class ObviousPOSTagger:\n",
    "    def __init__(self, word_tag_storage):\n",
    "        self.wts = word_tag_storage\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        tags = []\n",
    "        ### YOUR CODE HERE\n",
    "        # Get the sequence of tags corresponding to this sentence.\n",
    "        # If your algorithm never saw the word then return 'NN'.\n",
    "        for x in sent:\n",
    "            \n",
    "                if x in self.wts.model.keys():\n",
    "                    key = self.wts[x].most_common(1)[0]\n",
    "                    tags.append(key)\n",
    "                else:\n",
    "                    tags.append('NN')\n",
    "            \n",
    "        ### END YOUR CODE\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'s': {'t': 0.3333333333333333, 'l': 0.6666666666666666}})\n",
      "b\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-5bdd9513297e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mObviousPOSTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwtm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-100-2d3c78fedd5a>\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, sent)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                     \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "ts = [ [('s','t')],[('s','l')],[('s','l')]]\n",
    "wtm = WordTagModel(ts)\n",
    "print(wtm.model)\n",
    "l = {'a':3, 'b':2}\n",
    "am = max(l)\n",
    "print(am)\n",
    "b = ObviousPOSTagger(wtm)\n",
    "s = ['s']\n",
    "print(b.tag(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим корпус, обучим на нем нашу модель и создадим POS-теггер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't forget to download conll2000 corpus using nltk.download()\n",
    "nltk.download('conll2000')\n",
    "train_sents = conll2000.tagged_sents()[:8000]\n",
    "test_sents = conll2000.tagged_sents()[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wtm = WordTagModel(train_sents)\n",
    "obv_postagger = ObviousPOSTagger(wtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, все ли работает. Для этого Вам предоставляется простенький классик **SanityChecker**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SanityChecker:\n",
    "    def __init__(self, test_phrases, test_answers):\n",
    "        self.test_phrases = test_phrases\n",
    "        self.test_answers = test_answers\n",
    "    \n",
    "    def check(self, postagger):\n",
    "        for phrase, answer in zip(self.test_phrases, self.test_answers):\n",
    "            your_answer = postagger.tag(phrase.split())\n",
    "            if your_answer != answer:\n",
    "                print('Real answer = {}.\\nYour answer = {}.'.format(answer, your_answer))\n",
    "                print('Correct your code!')\n",
    "                return\n",
    "        print('Sanity check passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-0b43ab7138f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0msanity_cheker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSanityChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_answers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0msanity_cheker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobv_postagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-c20bcf80f666>\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, postagger)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpostagger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mphrase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_phrases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_answers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0myour_answer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0myour_answer\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Real answer = {}.\\nYour answer = {}.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myour_answer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-6ccc56ccc93c>\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, sent)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                     \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                     \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "test_phrases = [\n",
    "    'The students model the language .',\n",
    "    'They present excellent results .',\n",
    "    'The fitting is perfect .',\n",
    "    'Students master the craft of computer programming .',\n",
    "]\n",
    "test_answers = [\n",
    "    [('The', 'DT'), ('students', 'NNS'), ('model', 'NN'), ('the', 'DT'), ('language', 'NN'), ('.', '.')],\n",
    "    [('They', 'PRP'), ('present', 'JJ'), ('excellent', 'NN'), ('results', 'NNS'), ('.', '.')],\n",
    "    [('The', 'DT'), ('fitting', 'JJ'), ('is', 'VBZ'), ('perfect', 'JJ'), ('.', '.')],\n",
    "    [('Students', 'NN'), ('master', 'NN'), ('the', 'DT'), ('craft', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('programming', 'NN'), ('.', '.')],\n",
    "]\n",
    "\n",
    "sanity_cheker = SanityChecker(test_phrases, test_answers)\n",
    "sanity_cheker.check(obv_postagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При POS-теггинге естественно измерять точность посредством **accuracy**. Напишите функцию, ее реализующую."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(test_sents, postagger):\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Барабанная дробь! С какой же точностью в итоге рабоает наш пос-теггер?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of ObviousPOSTagger:', accuracy(test_sents, obv_postagger))\n",
    "print('Accuracy of ObviousPOSTagger on last 200 sents:', accuracy(test_sents[-200:], obv_postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<font color='red'>Ответьте на следующие вопросы (внутри ipython ноутбука):</font>\n",
    "\n",
    "Каким образом можно было бы улучшить **ObviousPOSTagger**? \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бесконтекстный POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторым разминочным этапом будет написание бесконтекстного pos-теггера. Мы стараемся максимизировать вероятность слова, считая, что на это влияет лишь его тег. Принцип его работы можно записать следующей формулой:\n",
    "$$\n",
    "    tag(w) = \\arg \\max_{i \\in 1 .. |Tags| } P(w \\mid tag_i)P(tag_i),\n",
    "$$\n",
    "\n",
    "Минуса с тем, что незнакомому слову мы не сможем поставить тег, уже нет, ибо мы можем некоторым образом сгладить $P(word \\mid tag)$ и избавиться от нулевых вероятностей.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализуйте:\n",
    "1. Класс **CustomNormalizer**, осуществляющий кастомную нормализацию Counter'ов (придумайте что-нибудь на Ваш вкус).\n",
    "2. Класс **EmissionModel**, хранящий для каждого тега вероятности быть присвоенным тому или иному слову.\n",
    "3. Класс **TransitionModel**, отвечающий за вероятность $P(tag_i)$.\n",
    "4. Класс **ObviousPOSTagger**, сопоставляющий последовательности слов последовательность тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomNormalizer:\n",
    "    ### YOUR CODE HERE\n",
    "    def __init__(self):\n",
    "        # Maybe you will add some new parameters to normalizer.\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    def normalize(self, counter):\n",
    "        ### YOUR CODE HERE\n",
    "        # You can make some custom normaliztion, but add all additional\n",
    "        # parameters in the initialization section.\n",
    "        # In fact it must not be even a probability distribution :).\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        \n",
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        ### YOUR CODE HERE\n",
    "        # For each tag add Counter with words corresponding to this tag.\n",
    "        # Don't forget to add UNK token and to normalize!\n",
    "        # This model should be able to give probabilities P(word | tag).\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.1\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]\n",
    "\n",
    "    \n",
    "class TransitionModel:\n",
    "    def __init__(self, tag_seqs, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = Counter()\n",
    "        ### YOUR CODE HERE\n",
    "        # Create counter with tag counts.\n",
    "        # Don't forget to normalize!\n",
    "        # This model should be able to give probabilities P(tag).\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, tag):\n",
    "        return self.model[tag]\n",
    "\n",
    "    \n",
    "class ContextlessPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        tags = []\n",
    "        ### YOUR CODE HERE\n",
    "        # Get the sequence of tags corresponding to this sentence.\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачаем корпус и обучим на нем **EmissionModel** и **TransitionModel** с **BaseNormalizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "em = EmissionModel(train_sents)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents])\n",
    "cl_postagger = ContextlessPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверочка, все ли работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_phrases = [\n",
    "    'The students model the language .',\n",
    "    'They present excellent results .',\n",
    "    'The fitting is perfect .',\n",
    "    'Students master the craft of computer programming .',\n",
    "]\n",
    "test_answers = [\n",
    "    [('The', 'DT'), ('students', 'NNS'), ('model', 'NN'), ('the', 'DT'), ('language', 'NN'), ('.', '.')],\n",
    "    [('They', 'PRP'), ('present', 'JJ'), ('excellent', 'NN'), ('results', 'NNS'), ('.', '.')],\n",
    "    [('The', 'DT'), ('fitting', 'JJ'), ('is', 'VBZ'), ('perfect', 'JJ'), ('.', '.')],\n",
    "    [('Students', 'NN'), ('master', 'NN'), ('the', 'DT'), ('craft', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('programming', 'NN'), ('.', '.')],\n",
    "]\n",
    "\n",
    "sanity_cheker = SanityChecker(test_phrases, test_answers)\n",
    "sanity_cheker.check(cl_postagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде как Ваш POS-теггер работает корректно, теперь можно опробовать его с кастомной нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Change BaseNormalizer by your CustomNormalizer.\n",
    "em = EmissionModel(train_sents, BaseNormalizer())\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents], BaseNormalizer())\n",
    "cl_postagger = ContextlessPOSTagger(em, tm)\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность нашего бесконтекстного пос-теггера.  \n",
    "В идеале она может достичь порога **92**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of ContextlessPOSTagger:', accuracy(test_sents, cl_postagger))\n",
    "print('Accuracy of ContextlessPOSTagger on last 200 sents:', accuracy(test_sents[-200:], cl_postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS-tagging на основе алгоритма Витерби"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну вот, мы и добрались! \n",
    "\n",
    "Картинка для ~~привлечения внимания~~ понимания скрытых марковских моделей ([HMM](https://en.wikipedia.org/wiki/Hidden_Markov_model)).\n",
    "![](http://i.imgur.com/BIRG7PM.png?)\n",
    "\n",
    "Тег будет отвечать за скрытую переменную (на рисунке $x(t)$), а слово из предложения за наблюдаемую переменную (на рисунке $y(t)$). В итоге нам понадобятся две модели:\n",
    "\n",
    "* Первая будет отвечать за вероятности $P(word \\mid tag)$.\n",
    "* Вторая же будет отвечать за вероятности $P(tag_n \\mid tag_1, \\dots, tag_{n-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "Прежде чем приступать к написанию POS-теггера на основе алгоритма Витерби, нужно написать **TransitionModel**, которая умеет выдавать вероятности $P(tag_2 | tag_1)$. Договоримся, что сглаживать ее будем посредством StupidBackoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tag_seqs, normalizer=BaseNormalizer(), multiplier=0.05):\n",
    "        self.normalizer = normalizer\n",
    "        self.mult = multiplier\n",
    "        # Some strange realization but nevertheless.\n",
    "        # In 1 you should store tag probabilities P(tag).\n",
    "        # In 2 you should store conditional tag probabilities P(tag_2 | tag_1)\n",
    "        # for each tag_1.\n",
    "        self.model = {1: Counter(), 2: defaultdict(Counter)}\n",
    "        ### YOUR CODE HERE\n",
    "        # Don't forget normalization.\n",
    "\n",
    "        ### END YOUR CODE\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __call__(self, tag, old_tag=None):\n",
    "        # Stupid backoff smoothing is sitting here.\n",
    "        if old_tag == None:\n",
    "            return self.model[1][tag]\n",
    "        if tag not in self.model[2][old_tag]:\n",
    "            return self.model[1][tag] * self.mult\n",
    "        return self.model[2][old_tag][tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Цель алгоритма Витерби является определение наиболее вероятной последовательности скрытых переменных $x_1, \\dots, x_T$. В нашем распоряжении есть:\n",
    "\n",
    "- пространство состояний $S$\n",
    "- начальные вероятности $\\pi_i$ нахождения в состоянии $i$\n",
    "- вероятностями $a_{i,j}$ перехода из состояния $i$ в состояние $j$\n",
    "- вероятности $P(y_i \\mid k), k \\in S$\n",
    "- наблюдаемые $y_1, \\dots, y_T$\n",
    "\n",
    "Несложно поверить, что наиболее вероятная последовательность состояний $x_1, \\dots, x_T$ задается рекуррентными соотношениями:\n",
    "\n",
    "- $V_{1,k} = P(y_1 \\mid k) \\cdot \\pi_k$\n",
    "- $V_{t,k} = P(y_t \\mid k) \\cdot \\max_{x \\in S} \\{a_{x,k} \\cdot V_{t-1,x}\\}$\n",
    "\n",
    "$V_{t, k}$ — наибольшая вероятность последовательности состояний длины $t$, заканчивающееся состоянием $k$. В это несложно поверить по одной простой причине, что $x_t$ зависит только от $x_{t-1}$, поэтому нам нужно помнить лишь наибольшие вероятности попадания в каждое из состояний на предыдущем шаге. На этом же основании можно найти не только вероятности, но и самый вероятный путь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HMMPOSTagger:\n",
    "    def __init__(self, transition_model, emission_model):\n",
    "        self.tm = transition_model\n",
    "        self.em = emission_model\n",
    "        \n",
    "    def tag(self, sent):\n",
    "        # Viterbi algorithm should be realized here.\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем **EmissionModel** и **TransitionModel** с **BaseNormalizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "em = EmissionModel(train_sents)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents])\n",
    "hmm_postagger = HMMPOSTagger(tm, em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверочка, все ли работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_phrases = [\n",
    "    'The students model the language .',\n",
    "    'They present excellent results .',\n",
    "    'The fitting is perfect .',\n",
    "    'Students master the craft of computer programming .',\n",
    "]\n",
    "test_answers = [\n",
    "    [('The', 'SYM'), ('students', 'FW'), ('model', 'FW'), ('the', 'DT'), ('language', 'NN'), ('.', '.')],\n",
    "    [('They', 'SYM'), ('present', 'FW'), ('excellent', 'FW'), ('results', 'FW'), ('.', '.')],\n",
    "    [('The', 'SYM'), ('fitting', 'FW'), ('is', 'FW'), ('perfect', 'FW'), ('.', '.')],\n",
    "    [('Students', 'SYM'), ('master', 'FW'), ('the', 'DT'), ('craft', 'NN'), ('of', 'IN'), ('computer', 'NN'), ('programming', 'NN'), ('.', '.')],\n",
    "]\n",
    "\n",
    "sanity_cheker = SanityChecker(test_phrases, test_answers)\n",
    "sanity_cheker.check(hmm_postagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вроде как Ваш POS-теггер работает корректно, теперь можно опробовать его с кастомной нормализацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "em = EmissionModel(train_sents, BaseNormalizer())\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents], BaseNormalizer())\n",
    "hmm_postagger = HMMPOSTagger(tm, em)\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем точность нашего бесконтекстного пос-теггера.  \n",
    "В идеале она может достичь порога **93.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of HMMPOSTagger:', accuracy(test_sents[-200:], hmm_postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Развернутый HMM-POStagger\n",
    "Суть крайне проста, давайте будем считать, что не предыдущий тег влияет на следующий за ним, а наоборот. Посмотрим, что при этом у нас выйдет. Реализуйте эту идею в **RevHMMPOSTagger** просто переписав лишь функцию **tag**.\n",
    "\n",
    "**Примечание**. Для того, чтобы вызвать какую-то функцию у класса от которого Вы наследовались используйте следующую конструкцию:\n",
    "```python\n",
    "class Father:\n",
    "    def hit(self):\n",
    "        print('Take this!')\n",
    "\n",
    "class Son(Father):\n",
    "    def hit(self):\n",
    "        print('Call father.')\n",
    "        super(Son, self).hit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RevHMMPOSTagger(HMMPOSTagger):\n",
    "    def tag(self, sent):\n",
    "        ### YOUR CODE HERE\n",
    "        # Just reverse the sentence and then reverse the output.\n",
    "\n",
    "        ### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем модель с кастомными нормализаторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "# Don't forget to reverse train.\n",
    "em = EmissionModel([sent[::-1] for sent in train_sents], BaseNormalizer())\n",
    "tm = TransitionModel([[tag for word, tag in sent][::-1] for sent in train_sents], BaseNormalizer())\n",
    "rev_hmm_postagger = RevHMMPOSTagger(tm, em)\n",
    "### END YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно, получится что-то лучше?  \n",
    "В идеале можно получить что-то порядка **94.5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of HMMPOSTagger:', accuracy(test_sents[-200:], rev_hmm_postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Композиция POS-теггеров\n",
    "Напишем совершенно простенькую композицию POS-теггеров и поймем, получим ли мы от этого какую-то пользу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSTaggersComposition:\n",
    "    def __init__(self, pos_taggers):\n",
    "        self.pos_taggers = pos_taggers\n",
    "    \n",
    "    def vote(self, objects):\n",
    "        return Counter(objects).most_common(1)[0][0]\n",
    "    \n",
    "    def tag(self, sent):\n",
    "        tags = [[tag for word, tag in postagger.tag(sent)] for postagger in self.pos_taggers]\n",
    "        tags = [self.vote(cur_tags) for cur_tags in zip(*tags)]\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренируем..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "composition = POSTaggersComposition([hmm_postagger, rev_hmm_postagger, cl_postagger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Барабанная дробь!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of HMMPOSTagger:', accuracy(test_sents[-200:], composition))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В NLTK тоже есть pos-теггеры! Так как использовать их достаточно просто, сразу привожу кусочек кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "combined_bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Accuracy of UnigramTagger:', accuracy(test_sents[-200:], unigram_tagger))\n",
    "print('Accuracy of BigramTagger:', accuracy(test_sents[-200:], bigram_tagger))\n",
    "print('Accuracy of Unigram+BigramTagger:', accuracy(test_sents[-200:], combined_bigram_tagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, pos-теггеры семинаристов отработали лучше, чем NLTK-шные.  \n",
    "**Вывод:** не все хорошо, что библиотечное!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
